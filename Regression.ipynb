{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Regression**"
      ],
      "metadata": {
        "id": "BTx3El8n48jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Assignment Questions:**"
      ],
      "metadata": {
        "id": "dVVVExk5qSB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.What is Simple Linear Regression?**"
      ],
      "metadata": {
        "id": "Qz5GML0oqWr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: Simple Linear Regression (SLR) is a method to model the relationship between two variables by fitting a straight line to the data. It predicts the value of a dependent variable (Y) based on an independent variable (X) using the equation:\n",
        "\n",
        "\\[\n",
        "**Y = \\beta_0 + \\beta_1X + \\epsilon**\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\beta_0 \\) is the intercept (Y value when X = 0),\n",
        "- \\( \\beta_1 \\) is the slope (rate of change of Y with X),\n",
        "- \\( \\epsilon \\) is the error term (the difference between actual and predicted Y).\n",
        "\n",
        "SLR assumes a linear relationship and is used for predicting one variable based on another."
      ],
      "metadata": {
        "id": "N5XCZze2qhsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.What are the key assumptions of Simple Linear Regression?**"
      ],
      "metadata": {
        "id": "1xbApdkSq9pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: The key assumptions of Simple Linear Regression are:\n",
        "\n",
        "1. **Linearity**: The relationship between the independent (X) and dependent (Y) variables is linear.\n",
        "2. **Independence**: The observations are independent of each other.\n",
        "3. **Homoscedasticity**: The variance of the errors is constant across all values of X.\n",
        "4. **Normality of errors**: The errors (differences between observed and predicted Y values) are normally distributed."
      ],
      "metadata": {
        "id": "9aRVes0CrDZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.What does the coefficient m represent in the equation Y=mX+c?**"
      ],
      "metadata": {
        "id": "E1oZJfwns3Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: In the equation \\( Y = mX + c \\), the coefficient \\( m \\) represents the **slope** of the line. It indicates the rate of change of the dependent variable \\( Y \\) for each unit increase in the independent variable \\( X \\).\n",
        "\n",
        "- If \\( m \\) is positive, \\( Y \\) increases as \\( X \\) increases.\n",
        "- If \\( m \\) is negative, \\( Y \\) decreases as \\( X \\) increases.\n",
        "  \n",
        "In simple terms, \\( m \\) tells you how steep the line is and how much \\( Y \\) changes for every 1-unit change in \\( X \\)."
      ],
      "metadata": {
        "id": "GexUfvgSs7pB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.What does the intercept c represent in the equation Y=mX+c?**"
      ],
      "metadata": {
        "id": "8vuT1X0YtBp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: In the equation \\( Y = mX + c \\), the intercept \\( c \\) represents the **y-intercept** of the line. It is the value of \\( Y \\) when the independent variable \\( X \\) equals zero.\n",
        "\n",
        "In other words, \\( c \\) tells you where the line crosses the Y-axis. If \\( X = 0 \\), then \\( Y = c \\)."
      ],
      "metadata": {
        "id": "UxhLOtRHtOGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.How do we calculate the slope m in Simple Linear Regression?**"
      ],
      "metadata": {
        "id": "1y8_kLNvtSqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: The slope \\( m \\) in Simple Linear Regression is calculated using the formula:\n",
        "\n",
        "\\[\n",
        "m = \\frac{n \\sum{XY} - \\sum{X} \\sum{Y}}{n \\sum{X^2} - (\\sum{X})^2}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( n \\) is the number of data points,\n",
        "- \\( \\sum{XY} \\) is the sum of the product of each pair of \\( X \\) and \\( Y \\),\n",
        "- \\( \\sum{X} \\) is the sum of all the values of \\( X \\),\n",
        "- \\( \\sum{Y} \\) is the sum of all the values of \\( Y \\),\n",
        "- \\( \\sum{X^2} \\) is the sum of the squares of all the values of \\( X \\).\n",
        "\n",
        "### In short:\n",
        "The slope \\( m \\) measures how much \\( Y \\) changes for a one-unit change in \\( X \\), and the formula uses the sums of the variables and their interactions to calculate it."
      ],
      "metadata": {
        "id": "Q3zwKpH9tYxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.What is the purpose of the least squares method in Simple Linear Regression?**"
      ],
      "metadata": {
        "id": "gWB7tCK7tef0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: The **least squares method** in Simple Linear Regression is used to find the best-fitting line by minimizing the sum of the squared differences between the observed values and the predicted values of the dependent variable \\( Y \\).\n",
        "\n",
        "### Purpose:\n",
        "- It aims to **minimize the error** or **residuals** (the vertical distance between the actual data points and the regression line).\n",
        "- By squaring the residuals, the method ensures that larger errors have a bigger impact on the line’s fit.\n",
        "- This approach results in the most accurate estimates of the slope \\( m \\) and intercept \\( c \\) by minimizing the sum of the squared errors, ensuring the line fits the data as closely as possible.\n",
        "\n",
        "In short, the least squares method helps to find the line that minimizes the overall error between the predicted and actual data points."
      ],
      "metadata": {
        "id": "LcvEh2jftlpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**"
      ],
      "metadata": {
        "id": "jfxP9F34twaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: In Simple Linear Regression, the **coefficient of determination (R²)** is a statistical measure that indicates how well the regression model explains the variability of the dependent variable \\( Y \\) based on the independent variable \\( X \\).\n",
        "\n",
        "### Interpretation of R²:\n",
        "- **R² value** ranges from 0 to 1.\n",
        "- **R² = 1**: The regression model explains all the variability in \\( Y \\). There is a perfect fit between the observed and predicted values.\n",
        "- **R² = 0**: The model explains none of the variability in \\( Y \\), meaning the independent variable \\( X \\) has no predictive power over \\( Y \\).\n",
        "- **0 < R² < 1**: The model explains some of the variability in \\( Y \\), but not all. A higher R² value indicates a better fit of the model to the data.\n",
        "\n",
        "In short, **R²** represents the percentage of the variation in \\( Y \\) that is explained by \\( X \\), with higher values indicating a stronger relationship between the variables."
      ],
      "metadata": {
        "id": "TpFKnfB7t056"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.What is Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "AmyiL7FvvcNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: **Multiple Linear Regression (MLR)** is an extension of Simple Linear Regression that models the relationship between a dependent variable and **two or more independent variables**. It is used when you want to predict or explain the value of a dependent variable based on multiple predictors.\n",
        "\n",
        "### General Equation:\n",
        "The equation for multiple linear regression is:\n",
        "\n",
        "\\[\n",
        "Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_nX_n + \\epsilon\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( Y \\) is the dependent variable,\n",
        "- \\( X_1, X_2, \\dots, X_n \\) are the independent variables (predictors),\n",
        "- \\( \\beta_0 \\) is the intercept (constant term),\n",
        "- \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the coefficients for the independent variables, showing how each \\( X \\) affects \\( Y \\),\n",
        "- \\( \\epsilon \\) is the error term (residuals).\n",
        "\n",
        "### Purpose:\n",
        "- **Prediction**: Predict the value of \\( Y \\) based on multiple predictors.\n",
        "- **Understanding Relationships**: Understand how multiple independent variables collectively influence the dependent variable.\n",
        "\n",
        "### Example:\n",
        "Predicting a person's salary (\\( Y \\)) based on factors like years of experience (\\( X_1 \\)), education level (\\( X_2 \\)), and age (\\( X_3 \\)).\n",
        "\n",
        "In summary, **Multiple Linear Regression** helps model complex relationships between a dependent variable and several independent variables."
      ],
      "metadata": {
        "id": "z-57aBVPvnr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.What is the main difference between Simple and Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "VhZ39VMIvyVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: The main difference between **Simple Linear Regression** and **Multiple Linear Regression** is the number of independent variables used to predict the dependent variable:\n",
        "\n",
        "1. **Simple Linear Regression**:\n",
        "   - Involves **one independent variable** (predictor).\n",
        "   - Models the relationship between a dependent variable \\( Y \\) and a single independent variable \\( X \\).\n",
        "   - Equation: \\( Y = \\beta_0 + \\beta_1X + \\epsilon \\)\n",
        "\n",
        "2. **Multiple Linear Regression**:\n",
        "   - Involves **two or more independent variables** (predictors).\n",
        "   - Models the relationship between a dependent variable \\( Y \\) and multiple independent variables \\( X_1, X_2, \\dots, X_n \\).\n",
        "   - Equation: \\( Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_nX_n + \\epsilon \\)\n",
        "\n",
        "### Key Difference:\n",
        "- **Simple Linear Regression** uses a single predictor, whereas **Multiple Linear Regression** uses multiple predictors to explain or predict the dependent variable."
      ],
      "metadata": {
        "id": "J3GSTodjwrZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.What are the key assumptions of Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "kC5Hp8uixMXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: The key assumptions of **Multiple Linear Regression** are similar to those of Simple Linear Regression, but with the added complexity of multiple predictors. These assumptions are important for ensuring the model produces reliable and valid results.\n",
        "\n",
        "1. **Linearity**: The relationship between the dependent variable (\\( Y \\)) and the independent variables (\\( X_1, X_2, \\dots, X_n \\)) is linear. This means that changes in the predictors lead to proportional changes in the dependent variable.\n",
        "\n",
        "2. **Independence**: The residuals (errors) are independent of each other. This means that there should be no correlation between the errors for different observations.\n",
        "\n",
        "3. **Homoscedasticity**: The variance of the errors is constant across all levels of the independent variables. This implies that the spread of residuals should be roughly the same across the range of predicted values.\n",
        "\n",
        "4. **No Multicollinearity**: The independent variables should not be highly correlated with each other. If predictors are highly correlated (multicollinearity), it can lead to unstable estimates of the regression coefficients and reduce the reliability of the model.\n",
        "\n",
        "5. **Normality of Errors**: The residuals (errors) should be normally distributed. This is important for valid hypothesis testing and confidence intervals for the coefficients.\n",
        "\n",
        "6. **No Autocorrelation**: In time series data, there should be no autocorrelation (i.e., no correlation between residuals at different time points).\n",
        "\n",
        "### In short:\n",
        "For valid results in **Multiple Linear Regression**, we assume linear relationships, independent and homoscedastic errors, no multicollinearity between predictors, and normally distributed residuals."
      ],
      "metadata": {
        "id": "P5mWwAJMxVWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**"
      ],
      "metadata": {
        "id": "3s9bIAOYxfZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: **Heteroscedasticity** refers to a situation in a regression model where the variance of the residuals (errors) is not constant across all levels of the independent variables. In other words, the spread or dispersion of the errors changes as the value of the independent variable(s) changes.\n",
        "\n",
        "### Effects of Heteroscedasticity on Multiple Linear Regression:\n",
        "\n",
        "1. **Inefficient Estimates**: Although the regression coefficients (\\( \\beta \\)) may still be unbiased, the standard errors of these estimates may be incorrect. This can lead to inefficient estimations and unreliable significance tests (e.g., t-tests), making it harder to determine the true relationships between variables.\n",
        "\n",
        "2. **Invalid Hypothesis Testing**: Heteroscedasticity affects the calculation of standard errors, which are used in hypothesis testing (like t-tests and F-tests). With incorrect standard errors, the p-values for these tests can be misleading, leading to incorrect conclusions about the significance of predictors.\n",
        "\n",
        "3. **Distorted Confidence Intervals**: Confidence intervals for the regression coefficients may be too narrow or too wide, depending on the nature of the heteroscedasticity, which means that we may overestimate or underestimate the precision of our predictions.\n",
        "\n",
        "4. **Inaccurate Predictions**: While heteroscedasticity doesn't bias the coefficients, it can still lead to inaccurate predictions because the model is less reliable when it comes to predicting the outcome for different values of the independent variables.\n",
        "\n",
        "### How to Detect Heteroscedasticity:\n",
        "- **Visual Inspection**: Plotting the residuals against the predicted values or independent variables. If the spread of residuals increases or decreases with the predicted values, this is an indication of heteroscedasticity.\n",
        "- **Breusch-Pagan Test** or **White's Test**: Statistical tests used to formally detect heteroscedasticity.\n",
        "\n",
        "### How to Address Heteroscedasticity:\n",
        "1. **Transformation of Variables**: Applying a transformation (such as a log transformation) to the dependent or independent variables can sometimes stabilize the variance.\n",
        "2. **Weighted Least Squares**: A method that adjusts the estimation procedure to account for heteroscedasticity by giving more weight to observations with smaller errors.\n",
        "3. **Robust Standard Errors**: Use robust standard errors (also called heteroscedasticity-consistent standard errors) that adjust for the presence of heteroscedasticity, providing more reliable estimates of standard errors.\n",
        "\n",
        "### In short:\n",
        "Heteroscedasticity occurs when the variability of errors changes across different levels of the independent variable(s). It can distort hypothesis tests and lead to inefficient estimates, making the model less reliable. Addressing it requires detection and potentially adjusting the model or using robust methods."
      ],
      "metadata": {
        "id": "ZaIAp-mZxmK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.How can you improve a Multiple Linear Regression model with high multicollinearity?**"
      ],
      "metadata": {
        "id": "_AUnm3Abx54t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: To improve a Multiple Linear Regression model with high multicollinearity, you can:\n",
        "\n",
        "1. **Remove Highly Correlated Predictors**: Use correlation matrices or VIF to identify and remove or combine correlated variables.\n",
        "2. **Use Principal Component Analysis (PCA)**: Combine correlated predictors into uncorrelated components.\n",
        "3. **Apply Regularization**: Use Ridge or Lasso regression to shrink coefficients and reduce multicollinearity.\n",
        "4. **Standardize the Data**: Standardizing variables can help when predictors are on different scales.\n",
        "5. **Increase Sample Size**: More data can reduce the effects of multicollinearity.\n",
        "6. **Use Domain Knowledge**: Prioritize important predictors based on expertise and relevance to the problem.\n",
        "\n",
        "These methods help stabilize the model and improve the reliability of estimates."
      ],
      "metadata": {
        "id": "uCi2aGI8yAag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.What are some common techniques for transforming categorical variables for use in regression models?**"
      ],
      "metadata": {
        "id": "19Fm7VfsyRWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: Here are common techniques for transforming categorical variables for regression:\n",
        "\n",
        "1. **One-Hot Encoding**: Creates binary columns for each category.\n",
        "2. **Label Encoding**: Assigns a unique integer to each category (good for ordinal data).\n",
        "3. **Ordinal Encoding**: Similar to label encoding, used for ordinal variables with a meaningful order.\n",
        "4. **Binary Encoding**: Converts categories into binary numbers, reducing dimensionality.\n",
        "5. **Frequency Encoding**: Replaces categories with their frequency of occurrence.\n",
        "6. **Target Encoding**: Replaces categories with the mean of the target variable for each category.\n",
        "7. **Hashing**: Maps categories into a fixed number of buckets, useful for high-cardinality data.\n",
        "\n"
      ],
      "metadata": {
        "id": "q9X4paaYypEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14.What is the role of interaction terms in Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "SrAunFzTywIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: In **Multiple Linear Regression**, **interaction terms** represent the combined effect of two or more independent variables on the dependent variable, beyond their individual effects. They allow the model to capture relationships where the effect of one predictor on the dependent variable depends on the value of another predictor.\n",
        "\n",
        "### Role of Interaction Terms:\n",
        "1. **Capture Combined Effects**: Interaction terms enable the model to consider how the relationship between the dependent variable and one predictor changes when another predictor changes.\n",
        "   \n",
        "   For example, in a model predicting salary, the effect of \"years of experience\" on salary might depend on the \"education level.\" An interaction term between these two variables would capture that combined effect.\n",
        "\n",
        "2. **Improve Model Accuracy**: By including interaction terms, the model can better reflect complex relationships and improve its predictive power.\n",
        "\n",
        "3. **Reveal Hidden Relationships**: Without interaction terms, the model might miss important relationships between predictors that jointly affect the dependent variable.\n",
        "\n",
        "### How to Include Interaction Terms:\n",
        "- Interaction terms are created by multiplying the independent variables. For example, if \\( X_1 \\) and \\( X_2 \\) are independent variables, an interaction term would be \\( X_1 \\times X_2 \\).\n",
        "\n",
        "   The regression equation with an interaction term looks like:\n",
        "   \\[\n",
        "   Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1 \\times X_2) + \\epsilon\n",
        "   \\]\n",
        "   "
      ],
      "metadata": {
        "id": "OgeCO_N0y0fY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "UPujazZ3zA10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: The interpretation of the **intercept** (\\( \\beta_0 \\)) in **Simple Linear Regression** and **Multiple Linear Regression** differs mainly due to the number of predictors involved.\n",
        "\n",
        "### 1. **In Simple Linear Regression**:\n",
        "   - The intercept represents the value of the dependent variable \\( Y \\) when the independent variable \\( X \\) is **zero**.\n",
        "   - **Interpretation**: It is the point where the regression line crosses the Y-axis, i.e., when \\( X = 0 \\), \\( Y = \\beta_0 \\).\n",
        "   - Example: If the model is \\( Y = \\beta_0 + \\beta_1X \\), the intercept \\( \\beta_0 \\) is the value of \\( Y \\) when \\( X = 0 \\).\n",
        "\n",
        "### 2. **In Multiple Linear Regression**:\n",
        "   - The intercept represents the value of the dependent variable \\( Y \\) when **all independent variables** are equal to zero.\n",
        "   - **Interpretation**: It is the predicted value of \\( Y \\) when \\( X_1 = X_2 = \\dots = X_n = 0 \\).\n",
        "   - **Important**: In real-world scenarios, it might not make sense for all predictors to be zero simultaneously, especially if some predictors cannot logically be zero (e.g., age, income).\n",
        "   - Example: For a model \\( Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_nX_n \\), the intercept \\( \\beta_0 \\) is the value of \\( Y \\) when all \\( X_1, X_2, \\dots, X_n \\) are zero.\n",
        "\n",
        "### Key Difference:\n",
        "- **Simple Linear Regression**: The intercept is the value of \\( Y \\) when \\( X = 0 \\).\n",
        "- **Multiple Linear Regression**: The intercept is the value of \\( Y \\) when **all predictors** are 0.\n",
        "\n",
        "In multiple regression, the intercept may not always have a meaningful or realistic interpretation, depending on the context of the predictors."
      ],
      "metadata": {
        "id": "W5vA08QZz41j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16.What is the significance of the slope in regression analysis, and how does it affect predictions?**"
      ],
      "metadata": {
        "id": "GarrJAFQ0DIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: In regression analysis, the **slope** (often denoted as \\( m \\) or \\( \\beta_1 \\)) represents the rate of change in the **dependent variable** (\\( Y \\)) for a **one-unit change** in the **independent variable** (\\( X \\)). It describes the strength and direction of the relationship between \\( X \\) and \\( Y \\).\n",
        "\n",
        "### Significance of the Slope:\n",
        "1. **Magnitude of Change**: The slope indicates how much the dependent variable is expected to change when the independent variable increases by one unit.\n",
        "   - For example, if the slope \\( \\beta_1 = 2 \\), then for each 1-unit increase in \\( X \\), \\( Y \\) will increase by 2 units.\n",
        "\n",
        "2. **Direction of Relationship**:\n",
        "   - **Positive Slope**: If the slope is positive (\\( \\beta_1 > 0 \\)), it means that as \\( X \\) increases, \\( Y \\) also increases (a positive relationship).\n",
        "   - **Negative Slope**: If the slope is negative (\\( \\beta_1 < 0 \\)), it means that as \\( X \\) increases, \\( Y \\) decreases (a negative relationship).\n",
        "\n",
        "3. **Prediction**: The slope directly influences how well the model predicts the dependent variable \\( Y \\). A larger absolute slope indicates a stronger relationship between \\( X \\) and \\( Y \\), leading to larger changes in \\( Y \\) as \\( X \\) changes.\n",
        "\n",
        "### How It Affects Predictions:\n",
        "- In **Simple Linear Regression** (\\( Y = \\beta_0 + \\beta_1X \\)), the slope \\( \\beta_1 \\) determines the steepness of the regression line. A steeper slope (higher absolute value) means predictions of \\( Y \\) will change more rapidly as \\( X \\) changes.\n",
        "- In **Multiple Linear Regression** (\\( Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_nX_n \\)), each slope coefficient represents the effect of its respective predictor on \\( Y \\), assuming all other predictors are held constant. For example, \\( \\beta_1 \\) shows how \\( Y \\) changes with a 1-unit change in \\( X_1 \\), holding all other variables constant.\n"
      ],
      "metadata": {
        "id": "xxbdH0J_0LQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17.How does the intercept in a regression model provide context for the relationship between variables?**"
      ],
      "metadata": {
        "id": "FKf0Wh0I0VLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: The **intercept** in a regression model is the predicted value of the dependent variable (\\( Y \\)) when all independent variables (\\( X_1, X_2, \\dots, X_n \\)) are zero. It provides a starting value or baseline for the model.\n",
        "\n",
        "- In **simple linear regression** (\\( Y = \\beta_0 + \\beta_1 X \\)), the intercept \\( \\beta_0 \\) is the value of \\( Y \\) when \\( X = 0 \\).\n",
        "- In **multiple linear regression** (\\( Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n \\)), the intercept \\( \\beta_0 \\) represents the value of \\( Y \\) when all \\( X_1, X_2, \\dots, X_n \\) are zero.\n",
        "\n",
        "While it gives context for the model, the intercept may not always be meaningful if a zero value for all predictors is unrealistic in practice."
      ],
      "metadata": {
        "id": "OSduE7N60cNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18.What are the limitations of using R² as a sole measure of model performance?**"
      ],
      "metadata": {
        "id": "PQalNAR30zOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:Using \\( R^2 \\) (R-squared) as the sole measure of model performance can be limiting because it doesn't capture the full complexity of a model's predictive ability. Here are some key limitations:\n",
        "\n",
        "1. **Doesn't Indicate Model Accuracy**:\n",
        "   \\( R^2 \\) measures how well the model explains the variance in the data, but it doesn’t necessarily show how accurately the model predicts the target values. A high \\( R^2 \\) does not mean that the model is accurate in its predictions, just that it fits the data well.\n",
        "\n",
        "2. **Sensitive to Outliers**:\n",
        "   \\( R^2 \\) is highly influenced by outliers. A few extreme values can significantly inflate or deflate the statistic, even if the overall model performance is poor.\n",
        "\n",
        "3. **Overfitting**:\n",
        "   A model can achieve a high \\( R^2 \\) by overfitting the data, particularly in cases with many predictors (features). Overfitting means the model is capturing noise rather than underlying patterns, which reduces its ability to generalize to new, unseen data.\n",
        "\n",
        "4. **Non-Linear Relationships**:\n",
        "   \\( R^2 \\) assumes a linear relationship between the predictors and the target variable. If the relationship is non-linear, \\( R^2 \\) may give a misleading sense of how well the model is performing.\n",
        "\n",
        "5. **Doesn't Capture Model Complexity**:\n",
        "   \\( R^2 \\) does not account for the number of predictors or model complexity. A model with more features may have a higher \\( R^2 \\), but that doesn’t necessarily mean it is better. Adjusted \\( R^2 \\) can help here by considering model complexity.\n",
        "\n",
        "6. **Not Robust to Different Types of Data**:\n",
        "   For some types of data or predictive tasks, like classification, \\( R^2 \\) is not an appropriate measure. For instance, \\( R^2 \\) is typically not used for classification tasks, where accuracy, precision, recall, or F1 score are more informative.\n",
        "\n",
        "7. **No Insight into Prediction Error**:\n",
        "   \\( R^2 \\) doesn’t tell you about the size or distribution of prediction errors. A model with a high \\( R^2 \\) might still have significant prediction errors that could be problematic in certain applications.\n",
        "\n",
        "8. **Assumes Homoscedasticity**:\n",
        "   \\( R^2 \\) assumes homoscedasticity, meaning the variance of residuals is constant across all levels of the independent variables. If the data shows heteroscedasticity (variance of errors changes with the predictors), \\( R^2 \\) can give a false impression of model performance.\n",
        "\n",
        "In summary, while \\( R^2 \\) is a valuable metric for some contexts, relying on it alone can lead to an incomplete or misleading evaluation of model performance. It’s important to complement \\( R^2 \\) with other metrics (like adjusted \\( R^2 \\), Mean Absolute Error, Root Mean Squared Error, or cross-validation scores) to get a fuller picture of how well your model is performing."
      ],
      "metadata": {
        "id": "T5aSJpWk04H6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19.How would you interpret a large standard error for a regression coefficient?**"
      ],
      "metadata": {
        "id": "tA4MAPdrd3Th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: A **large standard error** for a regression coefficient indicates uncertainty about the true value of that coefficient in the population. Specifically, it suggests that there is a lot of variability in the estimate of that coefficient, making it less reliable. Here's how you can interpret this:\n",
        "\n",
        "1. **Low Precision of the Estimate**:\n",
        "   The standard error measures how much the estimated coefficient varies across different samples of data. A large standard error means that the estimate of the coefficient could differ significantly if you were to collect a different sample of data. In other words, the coefficient estimate is less precise.\n",
        "\n",
        "2. **Possible Lack of Significance**:\n",
        "   A large standard error relative to the coefficient itself can reduce the statistical significance of the predictor. The significance of the coefficient is typically tested using a **t-statistic**, which is the ratio of the coefficient to its standard error. A larger standard error results in a smaller t-statistic, which in turn makes it less likely that the coefficient is significantly different from zero (i.e., less likely to show a meaningful relationship between the predictor and the outcome).\n",
        "\n",
        "3. **Potential Multicollinearity**:\n",
        "   If you have multiple predictors in your regression model and one or more of them are highly correlated with each other, it can lead to **multicollinearity**. Multicollinearity makes it difficult to isolate the effect of each predictor on the outcome, which can increase the standard errors of the regression coefficients. This means that the estimates are more volatile and less stable across samples.\n",
        "\n",
        "4. **Small Sample Size**:\n",
        "   A large standard error can also indicate that the sample size is too small to accurately estimate the regression coefficients. In small samples, the variability in coefficient estimates is naturally higher. Increasing the sample size typically leads to smaller standard errors and more precise estimates.\n",
        "\n",
        "5. **Model Misspecification**:\n",
        "   Large standard errors can also arise if the model is misspecified. For example, if you are not including important variables, or if you are assuming a wrong relationship (e.g., using a linear model when the true relationship is nonlinear), the regression coefficients may be poorly estimated, leading to large standard errors.\n",
        "\n",
        "### Example Interpretation:\n",
        "If you have a regression coefficient of 5 with a standard error of 10, the t-statistic would be:\n",
        "\n",
        "\\[\n",
        "\\text{t-statistic} = \\frac{\\text{Coefficient}}{\\text{Standard Error}} = \\frac{5}{10} = 0.5\n",
        "\\]\n",
        "\n",
        "This t-statistic is quite small, which suggests that the coefficient is not significantly different from zero, implying that the predictor may not have a meaningful relationship with the outcome.\n",
        "\n",
        "### How to Address a Large Standard Error:\n",
        "- **Increase Sample Size**: A larger sample can reduce the standard error, leading to more reliable estimates.\n",
        "- **Remove Multicollinearity**: Check for multicollinearity using variance inflation factors (VIF) and consider removing or combining highly correlated predictors.\n",
        "- **Reconsider Model Specification**: Ensure that your model is correctly specified and that you’re not omitting important variables.\n",
        "  \n",
        "In short, a large standard error reflects uncertainty and suggests that you should investigate your model’s reliability and potentially look for ways to improve the precision of your estimates."
      ],
      "metadata": {
        "id": "RynoqKYKd_IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?**"
      ],
      "metadata": {
        "id": "SOzbxM51eIpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: Heteroscedasticity can be identified in residual plots by looking for patterns where the spread of residuals increases or decreases as the fitted values change. A common sign is a **funnel shape** or **cone-like pattern** in the residuals vs. fitted values plot, where the residuals' variance grows larger or smaller with the predicted values.\n",
        "\n",
        "It’s important to address heteroscedasticity because it leads to **biased standard errors**, making statistical tests less reliable. This can result in incorrect conclusions about the significance of predictors. Additionally, it can cause **inefficient estimates**, reducing the precision of the model. Solutions include using **robust standard errors**, applying **weighted least squares (WLS)**, or transforming the dependent variable to stabilize variance."
      ],
      "metadata": {
        "id": "J88ZPxe2eYyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**"
      ],
      "metadata": {
        "id": "ya3fuCebe-Rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:If a **Multiple Linear Regression** model has a **high \\( R^2 \\)** but a **low adjusted \\( R^2 \\)**, it typically indicates that the model is **overfitting** the data. Here's what this means:\n",
        "\n",
        "- **High \\( R^2 \\)**: This suggests that the model explains a large proportion of the variance in the dependent variable. However, \\( R^2 \\) can increase simply by adding more predictors, even if they don't have meaningful relationships with the outcome. This is why it can be misleading.\n",
        "\n",
        "- **Low Adjusted \\( R^2 \\)**: The adjusted \\( R^2 \\) adjusts for the number of predictors in the model. Unlike \\( R^2 \\), it penalizes the inclusion of irrelevant predictors, meaning that if adding more variables doesn’t improve the model’s fit significantly, the adjusted \\( R^2 \\) will decrease. A low adjusted \\( R^2 \\) suggests that some of the predictors may not be contributing much to explaining the variability in the dependent variable and might be just adding noise.\n",
        "\n",
        "### Conclusion:\n",
        "- **High \\( R^2 \\) and low adjusted \\( R^2 \\)** usually signal **overfitting**, where the model has too many predictors or poorly chosen variables. It suggests that the model may not generalize well to new data, and it’s better to focus on improving the adjusted \\( R^2 \\) by removing unnecessary predictors."
      ],
      "metadata": {
        "id": "fZ9-lazgey5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22.Why is it important to scale variables in Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "29sdWm9rfHFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: Scaling variables in **Multiple Linear Regression** is important for several reasons:\n",
        "\n",
        "1. **Interpretability of Coefficients**:\n",
        "   - In multiple linear regression, the regression coefficients represent the change in the dependent variable for a one-unit change in the predictor. When variables have different scales (e.g., one in thousands and another between 0 and 1), the coefficients can be misleading. **Scaling** (e.g., standardizing variables) makes the coefficients comparable by putting all predictors on the same scale.\n",
        "\n",
        "2. **Avoiding Numerical Instability**:\n",
        "   - If the variables have vastly different scales, it can lead to numerical problems during optimization (e.g., difficulty in calculating coefficients or high variance in estimates). **Scaling** reduces the risk of such issues and ensures the model is more stable.\n",
        "\n",
        "3. **Improving Model Convergence**:\n",
        "   - Many regression algorithms use optimization techniques (like gradient descent) to find the best-fitting coefficients. If variables are on different scales, the algorithm might struggle to converge efficiently or take much longer to find the solution. **Scaling** helps the model converge faster.\n",
        "\n",
        "4. **Multicollinearity**:\n",
        "   - When predictor variables are highly correlated, the model might struggle to distinguish the individual effect of each variable. **Scaling** doesn’t directly fix multicollinearity, but it can make it easier to detect, especially when using methods like variance inflation factor (VIF).\n",
        "\n",
        "5. **Regularization Methods**:\n",
        "   - When using regularization techniques like **Ridge** or **Lasso regression**, scaling is crucial. These methods penalize the size of coefficients, and if the variables aren’t scaled, the penalty will disproportionately affect variables with larger scales, which can distort the model’s results.\n",
        "\n",
        "In short, scaling ensures that variables contribute equally to the model, stabilizes the calculations, and allows for better convergence and interpretation of results."
      ],
      "metadata": {
        "id": "ClTMsndJfLf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23.What is polynomial regression?**"
      ],
      "metadata": {
        "id": "hZdEblzhfT5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: **Polynomial regression** is an extension of linear regression that models the relationship between the independent variable(s) and the dependent variable using polynomial terms (e.g., \\(x^2, x^3\\)). This allows the model to fit curves to the data, capturing non-linear relationships. For example, a quadratic regression (degree 2) fits a parabolic curve. While it can improve model accuracy for non-linear data, using too high a polynomial degree can lead to **overfitting**, where the model fits noise in the data rather than general trends."
      ],
      "metadata": {
        "id": "_djR4hfYfYGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24.How does polynomial regression differ from linear regression?**"
      ],
      "metadata": {
        "id": "rPnGslD9fiF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: **Polynomial regression** differs from **linear regression** in how they model the relationship between the independent variable(s) and the dependent variable:\n",
        "\n",
        "1. **Model Form**:\n",
        "   - **Linear Regression**: Assumes a **linear relationship** between the predictors and the target variable. The model is of the form:\n",
        "     \\[\n",
        "     y = \\beta_0 + \\beta_1 x + \\epsilon\n",
        "     \\]\n",
        "     where \\(x\\) is the independent variable, and the relationship is represented by a straight line.\n",
        "\n",
        "   - **Polynomial Regression**: Extends linear regression by adding **higher-degree terms** (e.g., \\(x^2, x^3\\)) to model **non-linear relationships**. For example, a quadratic polynomial regression (degree 2) looks like:\n",
        "     \\[\n",
        "     y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon\n",
        "     \\]\n",
        "     This allows the model to fit curves (like parabolas) to the data.\n",
        "\n",
        "2. **Flexibility**:\n",
        "   - **Linear Regression**: Can only fit straight lines, so it’s limited to linear relationships.\n",
        "   - **Polynomial Regression**: Can fit curved relationships by adjusting the degree of the polynomial, making it more flexible in capturing complex patterns in the data.\n",
        "\n",
        "3. **Complexity**:\n",
        "   - **Linear Regression**: Simpler and easier to interpret because it models a straight-line relationship.\n",
        "   - **Polynomial Regression**: More complex, as it includes higher-degree terms and can become difficult to interpret as the degree increases.\n",
        "\n",
        "4. **Risk of Overfitting**:\n",
        "   - **Linear Regression**: Less prone to overfitting, especially with fewer predictors.\n",
        "   - **Polynomial Regression**: Higher-degree polynomials are more prone to **overfitting**, especially when the degree is too high, as the model might fit the noise in the data.\n",
        "\n",
        "In summary, polynomial regression can model non-linear relationships by adding polynomial terms, whereas linear regression is limited to fitting straight lines. However, polynomial regression requires careful selection of the polynomial degree to avoid overfitting."
      ],
      "metadata": {
        "id": "xM6dWz97fma-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25.When is polynomial regression used?**"
      ],
      "metadata": {
        "id": "fbU4NC5nftYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: **Polynomial regression** is used when the relationship between the independent variable(s) and the dependent variable is **non-linear**, but still follows a smooth, continuous pattern that can be captured by polynomial terms (e.g., squared, cubed terms). It is typically applied in the following situations:\n",
        "\n",
        "1. **Non-Linear Relationships**:\n",
        "   - When the data shows a **curved** or **non-linear** pattern (e.g., a parabolic shape), polynomial regression can fit a curve to better capture this relationship than linear regression can.\n",
        "\n",
        "2. **Data with Turning Points**:\n",
        "   - If the relationship between the variables changes direction (e.g., increases and then decreases), polynomial regression (especially quadratic or cubic) can capture these turning points.\n",
        "\n",
        "3. **Improving Model Fit**:\n",
        "   - When **linear regression** fails to adequately capture the data's trend or misses key patterns, polynomial regression can be used to improve the fit by adding higher-degree terms.\n",
        "\n",
        "4. **Predicting in Curved Trends**:\n",
        "   - Polynomial regression is often applied in cases like **economics, biology, physics**, and **engineering**, where many real-world relationships (e.g., growth rates, speed vs. time, etc.) follow non-linear trends.\n",
        "\n",
        "5. **When the Linear Model is Inadequate**:\n",
        "   - If the residuals from a linear regression model show a clear pattern (e.g., a curve), polynomial regression can provide a better fit by modeling the curvature.\n",
        "\n",
        "### Example:\n",
        "- **Sales vs. Advertising Spend**: In some cases, sales might increase with advertising spend but at a decreasing rate. A quadratic model (degree 2) could capture this **diminishing returns** effect better than a simple linear model.\n",
        "\n",
        "### Important Consideration:\n",
        "- **Overfitting**: Polynomial regression can lead to overfitting, especially with higher-degree polynomials, so it's essential to choose an appropriate degree for the polynomial.\n",
        "\n",
        "In summary, polynomial regression is used when the relationship between variables is non-linear, and you need a flexible model that can capture complex patterns."
      ],
      "metadata": {
        "id": "5Ue0CY6mfxyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**26.What is the general equation for polynomial regression?**"
      ],
      "metadata": {
        "id": "ehDf9Ltnf5Uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:The general equation for **polynomial regression** is an extension of the linear regression equation, where higher-degree polynomial terms of the independent variable(s) are included to model non-linear relationships. For a single predictor \\( x \\), the general equation is:\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\dots + \\beta_n x^n + \\epsilon\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( y \\) is the dependent variable (target).\n",
        "- \\( x \\) is the independent variable (predictor).\n",
        "- \\( \\beta_0 \\) is the intercept.\n",
        "- \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the coefficients for each polynomial term.\n",
        "- \\( x^2, x^3, \\dots, x^n \\) are the higher-degree terms of the independent variable \\( x \\) (depending on the degree of the polynomial).\n",
        "- \\( \\epsilon \\) is the error term (residuals).\n",
        "\n",
        "### For example:\n",
        "- **Linear regression** (degree 1) would have only \\( \\beta_1 x \\), and the equation would be:\n",
        "  \\[\n",
        "  y = \\beta_0 + \\beta_1 x + \\epsilon\n",
        "  \\]\n",
        "- **Quadratic regression** (degree 2) would have \\( \\beta_2 x^2 \\), and the equation would be:\n",
        "  \\[\n",
        "  y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon\n",
        "  \\]\n",
        "- **Cubic regression** (degree 3) would have \\( \\beta_3 x^3 \\), and the equation would be:\n",
        "  \\[\n",
        "  y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\epsilon\n",
        "  \\]\n",
        "\n",
        "The degree of the polynomial \\( n \\) determines how many terms (powers of \\( x \\)) are included in the model. Higher degrees allow the model to capture more complex, non-linear relationships but may also increase the risk of overfitting."
      ],
      "metadata": {
        "id": "ud_JSlRAf_oG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**27.Can polynomial regression be applied to multiple variables?**"
      ],
      "metadata": {
        "id": "jeA7v9A6gJGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:Yes, **polynomial regression** can be applied to **multiple variables** (i.e., multiple predictors). When multiple independent variables are involved, polynomial regression becomes a form of **multivariable polynomial regression**, where you include polynomial terms of the predictors, not just individual ones.\n",
        "\n",
        "The general equation for polynomial regression with multiple variables (predictors) is:\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\beta_5 x_2^2 + \\dots + \\beta_n x_2^k + \\epsilon\n",
        "\\]\n",
        "\n",
        "### Key Points:\n",
        "- The polynomial terms include not only the higher powers of each predictor (e.g., \\( x_1^2, x_2^2 \\)) but also interaction terms (e.g., \\( x_1 x_2 \\)).\n",
        "- The model can include terms for each predictor raised to a power (e.g., \\( x_1^2, x_2^2 \\), etc.) and interaction terms between predictors (e.g., \\( x_1 x_2, x_1 x_3 \\)).\n",
        "- The degree of the polynomial determines how many terms are included for each predictor, and it’s possible to model interactions between variables.\n",
        "\n",
        "### Example with Two Predictors:\n",
        "For two predictors \\( x_1 \\) and \\( x_2 \\), a quadratic polynomial regression (degree 2) might look like:\n",
        "\n",
        "\\[\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_2^2 + \\beta_5 x_1 x_2 + \\epsilon\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\beta_0 \\) is the intercept.\n",
        "- \\( \\beta_1, \\beta_2, \\dots \\) are the coefficients.\n",
        "- \\( x_1^2, x_2^2 \\) are the squared terms.\n",
        "- \\( x_1 x_2 \\) is the interaction term between the predictors.\n",
        "\n",
        "### Why Use Polynomial Regression with Multiple Variables:\n",
        "- **Non-linear relationships**: You may want to capture complex, non-linear relationships between multiple predictors and the dependent variable.\n",
        "- **Interaction effects**: Polynomial regression can help identify how variables interact, which can improve the model’s predictive power.\n",
        "\n",
        "### Considerations:\n",
        "- **Overfitting**: Higher-degree polynomial terms, especially with many predictors, can easily lead to overfitting if not managed properly.\n",
        "- **Complexity**: The model can become very complex as the number of predictors and the polynomial degree increase, making it harder to interpret and more computationally intensive.\n",
        "\n",
        "In summary, polynomial regression can be extended to multiple variables by including polynomial terms and interaction terms, allowing for more complex models that capture non-linear relationships between predictors and the outcome variable."
      ],
      "metadata": {
        "id": "uSPLqcz_gNNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: **Polynomial regression** has several limitations:\n",
        "\n",
        "1. **Overfitting**: Higher-degree polynomials can fit the training data too closely, capturing noise rather than the true relationship, leading to poor generalization on new data.\n",
        "\n",
        "2. **Complexity**: As the polynomial degree increases, the model becomes harder to interpret and understand, especially with multiple predictors and interaction terms.\n",
        "\n",
        "3. **Extrapolation Issues**: Polynomial models can behave unpredictably outside the range of the training data, producing unrealistic predictions.\n",
        "\n",
        "4. **Increased Computational Cost**: Higher-degree polynomials introduce more terms, making the model more computationally expensive to train and predict.\n",
        "\n",
        "5. **Multicollinearity**: When predictors are highly correlated, it can lead to unstable estimates of coefficients and unreliable predictions.\n",
        "\n",
        "6. **Sensitivity to Outliers**: Outliers can disproportionately affect polynomial regression, especially in higher-degree models, leading to distorted results.\n",
        "\n",
        "7. **Assumption of Smoothness**: Polynomial regression assumes the relationship is smooth, which might not be the case in real-world data, leading to inaccurate models.\n",
        "\n",
        "8. **Difficulty with Multiple Variables**: Adding more predictors increases the number of terms exponentially, making the model prone to overfitting and harder to manage.\n",
        "\n",
        "In short, while polynomial regression is useful for capturing non-linear relationships, it requires careful management of complexity to avoid overfitting, instability, and computational challenges."
      ],
      "metadata": {
        "id": "Qg351OZwgWoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?**"
      ],
      "metadata": {
        "id": "mz4bnG1xgnUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:When selecting the degree of a polynomial for regression, it's important to evaluate how well the model fits the data to avoid overfitting or underfitting. Here are several methods to assess the model fit:\n",
        "\n",
        "### 1. **Cross-Validation**:\n",
        "   - **K-fold cross-validation** involves splitting the data into \\(k\\) subsets, training the model on \\(k-1\\) subsets, and validating on the remaining one. This is repeated for all subsets, and the average performance is calculated.\n",
        "   - **Benefit**: Helps detect overfitting by evaluating how well the model generalizes to unseen data.\n",
        "\n",
        "### 2. **Adjusted \\(R^2\\)**:\n",
        "   - Unlike \\(R^2\\), which always increases with more predictors or polynomial terms, **adjusted \\(R^2\\)** penalizes the addition of unnecessary terms.\n",
        "   - **Benefit**: It helps to choose the degree that balances model complexity and goodness of fit, discouraging overfitting.\n",
        "\n",
        "### 3. **Akaike Information Criterion (AIC)**:\n",
        "   - AIC is a metric used to compare models, considering both the goodness of fit and the model complexity (number of parameters). Lower AIC values indicate better models.\n",
        "   - **Benefit**: Helps select the polynomial degree that fits well while avoiding overfitting by penalizing complex models.\n",
        "\n",
        "### 4. **Bayesian Information Criterion (BIC)**:\n",
        "   - Like AIC, BIC compares models but applies a stronger penalty for complexity. It is useful when the dataset is small.\n",
        "   - **Benefit**: It selects the polynomial degree that balances fit and complexity, often being more conservative than AIC.\n",
        "\n",
        "### 5. **Residual Plots**:\n",
        "   - Plot the residuals (errors) from the model against the fitted values. Ideally, residuals should appear randomly scattered, with no clear patterns or trends.\n",
        "   - **Benefit**: Helps assess whether the model has captured the underlying relationship. If residuals show patterns (e.g., a curve), the polynomial degree may be too low.\n",
        "\n",
        "### 6. **Out-of-Sample Performance**:\n",
        "   - Use a holdout test set (data not used during model training) to evaluate how the model performs on new, unseen data.\n",
        "   - **Benefit**: Ensures that the model generalizes well and does not overfit the training data.\n",
        "\n",
        "### 7. **Learning Curves**:\n",
        "   - Plot training and validation error against the polynomial degree. As the degree increases, the training error decreases, but the validation error may start to increase due to overfitting.\n",
        "   - **Benefit**: Shows the point where adding more complexity no longer improves generalization, indicating the optimal polynomial degree.\n",
        "\n",
        "### 8. **Mean Squared Error (MSE) or Root Mean Squared Error (RMSE)**:\n",
        "   - These metrics measure the average squared difference between the observed and predicted values. Lower values indicate better fit.\n",
        "   - **Benefit**: Helps evaluate how well the model predicts the target variable for different polynomial degrees.\n",
        "\n",
        "### In summary:\n",
        "To evaluate the model fit and select the optimal polynomial degree, use methods like **cross-validation**, **adjusted \\(R^2\\)**, **AIC/BIC**, **residual plots**, and **out-of-sample performance**. These techniques help ensure the model is not overfitting or underfitting while providing a good balance between complexity and prediction accuracy."
      ],
      "metadata": {
        "id": "IboPZfIcgyeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**30.Why is visualization important in polynomial regression?**"
      ],
      "metadata": {
        "id": "l6vPSvrVhM0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:**Visualization** is important in polynomial regression because it helps to:\n",
        "\n",
        "1. **Understand the Relationship**: It allows you to visually assess how well the polynomial fits the data, especially for identifying non-linear patterns.\n",
        "2. **Detect Overfitting**: By plotting the data and the regression curve, you can spot if the model is too complex, overfitting the data, and capturing noise.\n",
        "3. **Validate Model Assumptions**: Residual plots and other visualizations help check if the model assumptions (e.g., randomness of residuals) hold true.\n",
        "\n",
        "In short, visualization provides a clear way to evaluate the model’s fit, detect issues like overfitting, and ensure the regression model captures the true data trend."
      ],
      "metadata": {
        "id": "lM_8ZAPvhjte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**31.How is polynomial regression implemented in Python?**"
      ],
      "metadata": {
        "id": "40nq85I0hqDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:To implement polynomial regression in Python:\n",
        "\n",
        "Import libraries: Use NumPy, matplotlib, and scikit-learn.\n",
        "Prepare data: Create or load your dataset.\n",
        "Transform features: Use PolynomialFeatures to add polynomial terms to the independent variable.\n",
        "Fit model: Use LinearRegression from scikit-learn to fit the transformed data.\n",
        "Visualize results: Plot the original data and polynomial regression line."
      ],
      "metadata": {
        "id": "5zISpFm4husH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Prepare data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1, 4, 9, 16, 25])\n",
        "\n",
        "# Polynomial transformation\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, color='red')\n",
        "plt.plot(X, y_pred, color='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "4dDDfaNRh6Ys",
        "outputId": "0b82edaf-1e53-45c6-c972-2afae34be471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cimNuHvqeUD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7TsrtPtakGI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eyMuIYKq47w4"
      }
    }
  ]
}